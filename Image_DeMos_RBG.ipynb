{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import sklearn.feature_extraction\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from skimage import measure, io\n",
    "from skimage import transform\n",
    "import train_rgb \n",
    "import skimage\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "save_results = r'C:\\Users\\buggyr\\Mosaic_Experiments\\models'\n",
    "load_training = r'C:\\Users\\buggyr\\Mosaic_Experiments\\data\\external\\Training Images 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, UpSampling2D,Reshape, Conv2DTranspose\n",
    "from keras.optimizers import Adadelta, Nadam, RMSprop\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    20800     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, None, None, 64)    331840    \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 64)    331840    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 32)    2080      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 32)    25632     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, None, None, 32)    25632     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, None, None, 3)     2403      \n",
      "=================================================================\n",
      "Total params: 740,227\n",
      "Trainable params: 740,227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "f1 = 64\n",
    "f2 = 32\n",
    "f3 = 32\n",
    "ff = 3\n",
    "k1 = (9,9)\n",
    "k2 = (1,1)\n",
    "k3 = (5,5)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(f1,k1, padding = 'same', input_shape=(None, None ,4), activation = 'relu'))\n",
    "\n",
    "model.add(Conv2DTranspose(f1, k1, strides=(2, 2), padding='same'))\n",
    "\n",
    "model.add(Conv2D(f1,k1, padding = 'same', activation='relu'))\n",
    "\n",
    "model.add(Conv2D(f2,k2, padding = 'same',activation = 'relu'))\n",
    "\n",
    "model.add(Conv2D(f3,k3, padding = 'same',activation = 'relu'))\n",
    "\n",
    "model.add(Conv2D(f3,k3, padding = 'same',activation = 'relu'))\n",
    "\n",
    "model.add(Conv2D(ff,k3, padding = 'same',activation = 'tanh'))\n",
    "\n",
    "modsum = model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyname = \"_5layer_Transpose\"\n",
    "now=datetime.datetime.now()\n",
    "save_file=os.path.join(save_results,now.strftime(\"%Y-%m-%d %H-%M\")+keyname)\n",
    "os.mkdir(save_file)\n",
    "save_pred = os.path.join(save_file,'Epoch_Predictions')\n",
    "os.mkdir(save_pred)\n",
    "save_model = os.path.join(save_file,'Epoch_Models')\n",
    "os.mkdir(save_model)\n",
    "save_test = os.path.join(save_file,'Test_Results')\n",
    "os.mkdir(save_test)\n",
    "\n",
    "with open(os.path.join(save_file,'Model_Summary.txt'),'w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    \n",
    "#rmsprop = RMSprop(lr=0.0005, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "optimizer_func = Nadam(lr=0.00002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "loss_func='mse'\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=os.path.join(save_file,'TNSR_BRD'), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = keras.callbacks.CSVLogger(os.path.join(save_file,'training.log'), separator=',', append=False)\n",
    "epoch_predict = train_rgb.Save_predictions(save_pred)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(save_model,'model.{epoch:02d}-{loss:.2f}.hdf5'), monitor='loss')\n",
    "\n",
    "model.compile(optimizer=optimizer_func,loss=loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4107/4107 [==============================] - 1291s - loss: 0.0153  \n",
      "Epoch 2/200\n",
      "4107/4107 [==============================] - 1295s - loss: 0.0043  \n",
      "Epoch 3/200\n",
      "4107/4107 [==============================] - 1294s - loss: 0.0027  \n",
      "Epoch 4/200\n",
      "4107/4107 [==============================] - 1296s - loss: 0.0020  \n",
      "Epoch 5/200\n",
      "4107/4107 [==============================] - 1296s - loss: 0.0016  \n",
      "Epoch 6/200\n",
      "4107/4107 [==============================] - 1296s - loss: 0.0013  \n",
      "Epoch 7/200\n",
      "4107/4107 [==============================] - 1296s - loss: 0.0012  \n",
      "Epoch 8/200\n",
      "4107/4107 [==============================] - 1296s - loss: 0.0010  \n",
      "Epoch 9/200\n",
      "4107/4107 [==============================] - 1297s - loss: 9.7303e-04  \n",
      "Epoch 10/200\n",
      "4107/4107 [==============================] - 1298s - loss: 8.9412e-04  \n",
      "Epoch 11/200\n",
      "4107/4107 [==============================] - 1298s - loss: 8.2058e-04  \n",
      "Epoch 12/200\n",
      "4107/4107 [==============================] - 1298s - loss: 7.8779e-04  \n",
      "Epoch 13/200\n",
      "4107/4107 [==============================] - 1299s - loss: 7.3598e-04  \n",
      "Epoch 14/200\n",
      "4107/4107 [==============================] - 1299s - loss: 7.1393e-04  \n",
      "Epoch 15/200\n",
      "4107/4107 [==============================] - 1299s - loss: 6.5966e-04  \n",
      "Epoch 16/200\n",
      "4107/4107 [==============================] - 1300s - loss: 6.4377e-04  \n",
      "Epoch 17/200\n",
      "4107/4107 [==============================] - 1302s - loss: 6.2769e-04  \n",
      "Epoch 18/200\n",
      "4107/4107 [==============================] - 1303s - loss: 5.9840e-04  \n",
      "Epoch 19/200\n",
      "4107/4107 [==============================] - 1304s - loss: 5.9332e-04  \n",
      "Epoch 20/200\n",
      "4107/4107 [==============================] - 1304s - loss: 5.7173e-04  \n",
      "Epoch 21/200\n",
      "4107/4107 [==============================] - 1304s - loss: 5.5691e-04  \n",
      "Epoch 22/200\n",
      "4107/4107 [==============================] - 1305s - loss: 5.3398e-04  \n",
      "Epoch 23/200\n",
      "4107/4107 [==============================] - 1305s - loss: 5.1789e-04  \n",
      "Epoch 24/200\n",
      "4107/4107 [==============================] - 1305s - loss: 5.1222e-04  \n",
      "Epoch 25/200\n",
      "4107/4107 [==============================] - 1305s - loss: 4.9865e-04  \n",
      "Epoch 26/200\n",
      "4107/4107 [==============================] - 1305s - loss: 4.8690e-04  \n",
      "Epoch 27/200\n",
      "4107/4107 [==============================] - 1305s - loss: 4.7760e-04  \n",
      "Epoch 28/200\n",
      "4107/4107 [==============================] - 1300s - loss: 4.7591e-04  \n",
      "Epoch 29/200\n",
      "4107/4107 [==============================] - 1296s - loss: 4.7728e-04  \n",
      "Epoch 30/200\n",
      "4107/4107 [==============================] - 1296s - loss: 4.5694e-04  \n",
      "Epoch 31/200\n",
      "4107/4107 [==============================] - 1296s - loss: 4.4653e-04  \n",
      "Epoch 32/200\n",
      "4107/4107 [==============================] - 1296s - loss: 4.5216e-04  \n",
      "Epoch 33/200\n",
      "4107/4107 [==============================] - 1296s - loss: 4.5691e-04  \n",
      "Epoch 34/200\n",
      "4107/4107 [==============================] - 1297s - loss: 4.3356e-04  \n",
      "Epoch 35/200\n",
      "4107/4107 [==============================] - 1297s - loss: 4.3603e-04  \n",
      "Epoch 36/200\n",
      "4107/4107 [==============================] - 1297s - loss: 4.2297e-04  \n",
      "Epoch 37/200\n",
      "4107/4107 [==============================] - 1298s - loss: 4.1990e-04  \n",
      "Epoch 38/200\n",
      "4107/4107 [==============================] - 1299s - loss: 4.1661e-04  \n",
      "Epoch 39/200\n",
      "4107/4107 [==============================] - 1300s - loss: 4.0979e-04  \n",
      "Epoch 40/200\n",
      "4107/4107 [==============================] - 1300s - loss: 4.0464e-04  \n",
      "Epoch 41/200\n",
      "4107/4107 [==============================] - 1301s - loss: 4.0285e-04  \n",
      "Epoch 42/200\n",
      "4107/4107 [==============================] - 1302s - loss: 4.0468e-04  \n",
      "Epoch 43/200\n",
      "4107/4107 [==============================] - 1302s - loss: 3.9098e-04  \n",
      "Epoch 44/200\n",
      "4107/4107 [==============================] - 1302s - loss: 3.9067e-04  \n",
      "Epoch 45/200\n",
      "4107/4107 [==============================] - 1303s - loss: 3.9005e-04  \n",
      "Epoch 46/200\n",
      "4107/4107 [==============================] - 1303s - loss: 3.8235e-04  \n",
      "Epoch 47/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.9384e-04  \n",
      "Epoch 48/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.7897e-04  \n",
      "Epoch 49/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.8402e-04  \n",
      "Epoch 50/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.7568e-04  \n",
      "Epoch 51/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.7708e-04  \n",
      "Epoch 52/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.7023e-04  \n",
      "Epoch 53/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.6662e-04  \n",
      "Epoch 54/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.6387e-04  \n",
      "Epoch 55/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.6696e-04  \n",
      "Epoch 56/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.5817e-04  \n",
      "Epoch 57/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.6583e-04  \n",
      "Epoch 58/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.5594e-04  \n",
      "Epoch 59/200\n",
      "4107/4107 [==============================] - 1302s - loss: 3.6069e-04  \n",
      "Epoch 60/200\n",
      "4107/4107 [==============================] - 1295s - loss: 3.5588e-04  \n",
      "Epoch 61/200\n",
      "4107/4107 [==============================] - 1295s - loss: 3.5717e-04  \n",
      "Epoch 62/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.5679e-04  \n",
      "Epoch 63/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.4844e-04  \n",
      "Epoch 64/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.5445e-04  \n",
      "Epoch 65/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.5091e-04  \n",
      "Epoch 66/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.4692e-04  \n",
      "Epoch 67/200\n",
      "4107/4107 [==============================] - 1297s - loss: 3.4837e-04  \n",
      "Epoch 68/200\n",
      "4107/4107 [==============================] - 1297s - loss: 3.5876e-04  \n",
      "Epoch 69/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.3948e-04  \n",
      "Epoch 70/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.4372e-04  \n",
      "Epoch 71/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.4330e-04  \n",
      "Epoch 72/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.3950e-04  \n",
      "Epoch 73/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.3389e-04  \n",
      "Epoch 74/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.4073e-04  \n",
      "Epoch 75/200\n",
      "4107/4107 [==============================] - 1297s - loss: 3.2902e-04  \n",
      "Epoch 76/200\n",
      "4107/4107 [==============================] - 1297s - loss: 3.3811e-04  \n",
      "Epoch 77/200\n",
      "4107/4107 [==============================] - 1298s - loss: 3.3650e-04  \n",
      "Epoch 78/200\n",
      "4107/4107 [==============================] - 1298s - loss: 3.2890e-04  \n",
      "Epoch 79/200\n",
      "4107/4107 [==============================] - 1299s - loss: 3.3864e-04  \n",
      "Epoch 80/200\n",
      "4107/4107 [==============================] - 1300s - loss: 3.3247e-04  \n",
      "Epoch 81/200\n",
      "4107/4107 [==============================] - 1301s - loss: 3.2963e-04  \n",
      "Epoch 82/200\n",
      "4107/4107 [==============================] - 1302s - loss: 3.3086e-04  \n",
      "Epoch 83/200\n",
      "4107/4107 [==============================] - 1302s - loss: 3.2936e-04  \n",
      "Epoch 84/200\n",
      "4107/4107 [==============================] - 1303s - loss: 3.2217e-04  \n",
      "Epoch 85/200\n",
      "4107/4107 [==============================] - 1303s - loss: 3.2443e-04  \n",
      "Epoch 86/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.2709e-04  \n",
      "Epoch 87/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.2038e-04  \n",
      "Epoch 88/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.2345e-04  \n",
      "Epoch 89/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.2176e-04  \n",
      "Epoch 90/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.2048e-04  \n",
      "Epoch 91/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.1903e-04  \n",
      "Epoch 92/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.1568e-04  \n",
      "Epoch 93/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.1960e-04  \n",
      "Epoch 94/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.2113e-04  \n",
      "Epoch 95/200\n",
      "4107/4107 [==============================] - 1302s - loss: 3.1409e-04  \n",
      "Epoch 96/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.1618e-04  \n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4107/4107 [==============================] - 1296s - loss: 3.1564e-04  \n",
      "Epoch 98/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.1433e-04  \n",
      "Epoch 99/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.1187e-04  \n",
      "Epoch 100/200\n",
      "4107/4107 [==============================] - 1297s - loss: 3.1475e-04  \n",
      "Epoch 101/200\n",
      "4107/4107 [==============================] - 1296s - loss: 3.1046e-04  \n",
      "Epoch 102/200\n",
      "4107/4107 [==============================] - 1298s - loss: 3.0986e-04  \n",
      "Epoch 103/200\n",
      "4107/4107 [==============================] - 1298s - loss: 3.0696e-04  \n",
      "Epoch 104/200\n",
      "4107/4107 [==============================] - 1299s - loss: 3.0843e-04  \n",
      "Epoch 105/200\n",
      "4107/4107 [==============================] - 1299s - loss: 3.1174e-04  \n",
      "Epoch 106/200\n",
      "4107/4107 [==============================] - 1300s - loss: 3.0606e-04  \n",
      "Epoch 107/200\n",
      "4107/4107 [==============================] - 1301s - loss: 3.1024e-04  \n",
      "Epoch 108/200\n",
      "4107/4107 [==============================] - 1303s - loss: 3.0691e-04  \n",
      "Epoch 109/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.0553e-04  \n",
      "Epoch 110/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.0942e-04  \n",
      "Epoch 111/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0058e-04  \n",
      "Epoch 112/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0409e-04  \n",
      "Epoch 113/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0473e-04  \n",
      "Epoch 114/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0607e-04  \n",
      "Epoch 115/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0157e-04  \n",
      "Epoch 116/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.0429e-04  \n",
      "Epoch 117/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0298e-04  \n",
      "Epoch 118/200\n",
      "4107/4107 [==============================] - 1304s - loss: 3.0504e-04  \n",
      "Epoch 119/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0026e-04  \n",
      "Epoch 120/200\n",
      "4107/4107 [==============================] - 1306s - loss: 2.9847e-04  \n",
      "Epoch 121/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0063e-04  \n",
      "Epoch 122/200\n",
      "4107/4107 [==============================] - 1305s - loss: 3.0298e-04  \n",
      "Epoch 123/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.9795e-04  \n",
      "Epoch 124/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.9468e-04  \n",
      "Epoch 125/200\n",
      "4107/4107 [==============================] - 1298s - loss: 2.9585e-04  \n",
      "Epoch 126/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.9553e-04  \n",
      "Epoch 127/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.9620e-04  \n",
      "Epoch 128/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.9485e-04  \n",
      "Epoch 129/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.9379e-04  \n",
      "Epoch 130/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.9565e-04  \n",
      "Epoch 131/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.9256e-04  \n",
      "Epoch 132/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.9220e-04  \n",
      "Epoch 133/200\n",
      "4107/4107 [==============================] - 1297s - loss: 2.9101e-04  \n",
      "Epoch 134/200\n",
      "4107/4107 [==============================] - 1297s - loss: 2.9465e-04  \n",
      "Epoch 135/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.9414e-04  \n",
      "Epoch 136/200\n",
      "4107/4107 [==============================] - 1297s - loss: 2.9149e-04  \n",
      "Epoch 137/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.9159e-04  \n",
      "Epoch 138/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.8965e-04  \n",
      "Epoch 139/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.9077e-04  \n",
      "Epoch 140/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.8956e-04  \n",
      "Epoch 141/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.9060e-04  \n",
      "Epoch 142/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.9008e-04  \n",
      "Epoch 143/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.8726e-04  \n",
      "Epoch 144/200\n",
      "4107/4107 [==============================] - 1297s - loss: 2.8432e-04  \n",
      "Epoch 145/200\n",
      "4107/4107 [==============================] - 1298s - loss: 2.8987e-04  \n",
      "Epoch 146/200\n",
      "4107/4107 [==============================] - 1298s - loss: 2.8712e-04  \n",
      "Epoch 147/200\n",
      "4107/4107 [==============================] - 1298s - loss: 2.9059e-04  \n",
      "Epoch 148/200\n",
      "4107/4107 [==============================] - 1299s - loss: 2.8483e-04  \n",
      "Epoch 149/200\n",
      "4107/4107 [==============================] - 1300s - loss: 2.8505e-04  \n",
      "Epoch 150/200\n",
      "4107/4107 [==============================] - 1301s - loss: 2.8508e-04  \n",
      "Epoch 151/200\n",
      "4107/4107 [==============================] - 1302s - loss: 2.8779e-04  \n",
      "Epoch 152/200\n",
      "4107/4107 [==============================] - 1303s - loss: 2.8432e-04  \n",
      "Epoch 153/200\n",
      "4107/4107 [==============================] - 1304s - loss: 2.8738e-04  \n",
      "Epoch 154/200\n",
      "4107/4107 [==============================] - 1304s - loss: 2.8256e-04  \n",
      "Epoch 155/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8153e-04  \n",
      "Epoch 156/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8246e-04  \n",
      "Epoch 157/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8066e-04  \n",
      "Epoch 158/200\n",
      "4107/4107 [==============================] - 1304s - loss: 2.8398e-04  \n",
      "Epoch 159/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8400e-04  \n",
      "Epoch 160/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8162e-04  \n",
      "Epoch 161/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8245e-04  \n",
      "Epoch 162/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8177e-04  \n",
      "Epoch 163/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8012e-04  \n",
      "Epoch 164/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8195e-04  \n",
      "Epoch 165/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.8086e-04  \n",
      "Epoch 166/200\n",
      "4107/4107 [==============================] - 1302s - loss: 2.7799e-04  \n",
      "Epoch 167/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.7801e-04  \n",
      "Epoch 168/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.7682e-04  \n",
      "Epoch 169/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.7881e-04  \n",
      "Epoch 170/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.7696e-04  \n",
      "Epoch 171/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.7830e-04  \n",
      "Epoch 172/200\n",
      "4107/4107 [==============================] - 1297s - loss: 2.7795e-04  \n",
      "Epoch 173/200\n",
      "4107/4107 [==============================] - 1297s - loss: 2.7977e-04  \n",
      "Epoch 174/200\n",
      "4107/4107 [==============================] - 1297s - loss: 2.7603e-04  \n",
      "Epoch 175/200\n",
      "4107/4107 [==============================] - 1298s - loss: 2.7589e-04  \n",
      "Epoch 176/200\n",
      "4107/4107 [==============================] - 1299s - loss: 2.7565e-04  \n",
      "Epoch 177/200\n",
      "4107/4107 [==============================] - 1300s - loss: 2.7335e-04  \n",
      "Epoch 178/200\n",
      "4107/4107 [==============================] - 1301s - loss: 2.7494e-04  \n",
      "Epoch 179/200\n",
      "4107/4107 [==============================] - 1302s - loss: 2.7603e-04  \n",
      "Epoch 180/200\n",
      "4107/4107 [==============================] - 1302s - loss: 2.7424e-04  \n",
      "Epoch 181/200\n",
      "4107/4107 [==============================] - 1303s - loss: 2.7823e-04  \n",
      "Epoch 182/200\n",
      "4107/4107 [==============================] - 1303s - loss: 2.7343e-04  \n",
      "Epoch 183/200\n",
      "4107/4107 [==============================] - 1304s - loss: 2.7277e-04  \n",
      "Epoch 184/200\n",
      "4107/4107 [==============================] - 1304s - loss: 2.7722e-04  \n",
      "Epoch 185/200\n",
      "4107/4107 [==============================] - 1304s - loss: 2.7680e-04  \n",
      "Epoch 186/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.7171e-04  \n",
      "Epoch 187/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.7202e-04  \n",
      "Epoch 188/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.7584e-04  \n",
      "Epoch 189/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.7260e-04  \n",
      "Epoch 190/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.7397e-04  \n",
      "Epoch 191/200\n",
      "4107/4107 [==============================] - 1305s - loss: 2.7265e-04  \n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4107/4107 [==============================] - 1297s - loss: 2.7160e-04  \n",
      "Epoch 193/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.6911e-04  \n",
      "Epoch 194/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.7056e-04  \n",
      "Epoch 195/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.6979e-04  \n",
      "Epoch 196/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.6907e-04  \n",
      "Epoch 197/200\n",
      "4107/4107 [==============================] - 1295s - loss: 2.7101e-04  \n",
      "Epoch 198/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.7016e-04  \n",
      "Epoch 199/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.7109e-04  \n",
      "Epoch 200/200\n",
      "4107/4107 [==============================] - 1296s - loss: 2.6970e-04  \n",
      "{'loss': [0.015313834418603456, 0.0043366691051909892, 0.0027470428549378678, 0.0019988896510355621, 0.0015813126277539591, 0.0013328207285105706, 0.0011729391639133972, 0.0010372685754566615, 0.00097302984919545054, 0.00089412351295364509, 0.0008205763711780559, 0.00078778733235883032, 0.00073598113484077009, 0.00071392692086743887, 0.0006596557529145458, 0.00064377336065643304, 0.00062769417889624952, 0.00059839709191180019, 0.00059332196409569017, 0.00057173221084122841, 0.00055690755383133793, 0.00053397643874908559, 0.00051789142471095958, 0.0005122246718456384, 0.00049865493721703603, 0.00048689936012100539, 0.00047760205818101088, 0.00047590786380833798, 0.00047728419963991877, 0.00045694108165564225, 0.00044652806619316701, 0.00045216346783407026, 0.00045691411802676999, 0.00043356379527699501, 0.00043602579761096847, 0.00042297149458256366, 0.00041990497696352405, 0.00041661248479742385, 0.00040978955962528625, 0.00040463999949711088, 0.00040285385174661242, 0.00040468223178238507, 0.00039097545546384167, 0.00039067094463333241, 0.00039004863885948234, 0.00038234592048093841, 0.00039384490897674518, 0.00037896864649074709, 0.00038402424861024985, 0.00037568405409154503, 0.00037707932555144041, 0.00037023364332752057, 0.00036662295082925191, 0.00036387226060581899, 0.00036695842378254808, 0.00035817249802824237, 0.00036583134369955677, 0.00035593940690516245, 0.00036069181861393261, 0.00035587788363359121, 0.00035717009146568309, 0.00035679276027103697, 0.00034843957145894492, 0.00035445111524100742, 0.00035090877911776112, 0.00034691978172384901, 0.00034837422272377419, 0.00035876285276558654, 0.00033947759967869952, 0.00034372240336654982, 0.00034330222415495168, 0.00033950053163249135, 0.00033388988553119739, 0.00034073158455276046, 0.00032902006708715491, 0.0003381075959277211, 0.00033649884797152093, 0.00032890441126493225, 0.00033863657138641362, 0.00033246734645373557, 0.0003296337108618109, 0.00033085933019011985, 0.00032936005115100206, 0.00032217444533588546, 0.000324431987504089, 0.00032708567369508807, 0.00032038337787498503, 0.0003234495073836033, 0.00032175840041127505, 0.00032048012146108798, 0.00031902920458372303, 0.00031568310827741398, 0.00031959756918480321, 0.0003211276522341432, 0.00031409386558419686, 0.00031617507200976911, 0.00031564392952925289, 0.00031432764329374102, 0.00031187299960854263, 0.00031474980137890645, 0.00031046296610050326, 0.00030986114417404649, 0.00030696389513715385, 0.00030842682832672524, 0.00031174446660626461, 0.00030605774037452372, 0.0003102382270028962, 0.00030691153237062603, 0.00030553129606682454, 0.00030941613473133407, 0.00030058447014788629, 0.00030408667569635143, 0.00030473085591749933, 0.00030607447355434687, 0.00030156612859183891, 0.00030428742755880112, 0.00030297608111831052, 0.00030503594832679838, 0.0003002612099204485, 0.00029846756977695637, 0.00030063484852231543, 0.00030297858420983114, 0.00029795141663720897, 0.00029467617889879921, 0.00029584582609221339, 0.00029552618329391862, 0.0002961974844645987, 0.0002948515808820213, 0.00029379167236620276, 0.00029565065827788002, 0.00029255835408352829, 0.0002921971380552079, 0.00029101203391452185, 0.00029465125079126216, 0.00029414452468213144, 0.00029149366725749203, 0.00029158528167614245, 0.00028964908109883707, 0.00029076970609024066, 0.00028956027386492581, 0.00029060079499996724, 0.00029008040688971058, 0.00028725745351524198, 0.00028431954266963953, 0.00028987429009406782, 0.00028712472528650955, 0.00029058764476333851, 0.0002848263024258667, 0.00028504863454209155, 0.00028507590948610625, 0.00028778839426996615, 0.00028431605933372024, 0.00028737697529804853, 0.00028255991115988532, 0.00028152573717201096, 0.00028245622946277901, 0.00028065689511845164, 0.00028398233804640343, 0.00028399898044860153, 0.00028161649039595236, 0.00028244539527666409, 0.00028176957380880577, 0.0002801204160193254, 0.00028194612795417741, 0.00028085769280641307, 0.0002779862378657137, 0.00027800776015099389, 0.00027681959725317783, 0.00027881108767870719, 0.0002769569233104669, 0.00027830499314245065, 0.00027794752921935314, 0.00027977028772130066, 0.00027603339312966968, 0.00027588538447403076, 0.00027564637901551482, 0.00027334985690268942, 0.00027494438294208793, 0.00027602743132701477, 0.00027423893987886945, 0.0002782341667162314, 0.00027342812039707614, 0.00027277054600692991, 0.0002772176259084394, 0.00027680106617838192, 0.00027171290070013797, 0.0002720172894475369, 0.00027583573881174127, 0.00027259811153905374, 0.00027397490866227261, 0.00027264819757393877, 0.00027159867395872445, 0.00026911115879029254, 0.00027055711485261255, 0.00026979400584053233, 0.00026907317897715535, 0.0002710100239851585, 0.00027015611193132653, 0.00027108554615444655, 0.00026970060268031895]}\n"
     ]
    }
   ],
   "source": [
    "fls = len(os.listdir(load_training))\n",
    "\n",
    "                            #train_dir, patch_size, batch_size\n",
    "train_generator = train_rgb.train_generator_rgb(load_training,64,32)\n",
    "history = model.fit_generator(generator = train_generator,steps_per_epoch=3*fls,\n",
    "                              verbose = 1, epochs = 200,callbacks = [tbCallBack,csv_logger,\n",
    "                                epoch_predict, model_checkpoint])\n",
    "print(history.history)\n",
    "\n",
    "model.save(os.path.join(save_file,'DeMos_mod.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptch_sizes = [64, 128]\n",
    "for ptch_size in ptch_sizes:\n",
    "    print(str(ptch_size))\n",
    "    \n",
    "    now=datetime.datetime.now()\n",
    "    save_file=os.path.join(save_results,now.strftime(\"%Y-%m-%d %H-%M\"))\n",
    "    os.mkdir(save_file)\n",
    "    save_pred = os.path.join(save_file,'Epoch_Predictions')\n",
    "    os.mkdir(save_pred)\n",
    "\n",
    "    with open(os.path.join(save_file,'Model_Summary.txt'),'w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    #rmsprop = RMSprop(lr=0.0005, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    optimizer_func = Nadam(lr=0.00002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "    loss_func='mse'\n",
    "\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "    tbCallBack = keras.callbacks.TensorBoard(log_dir=os.path.join(save_file,'TNSR_BRD'), histogram_freq=0, write_graph=True, write_images=True)\n",
    "    csv_logger = keras.callbacks.CSVLogger(os.path.join(save_file,'training.log'), separator=',', append=False)\n",
    "    epoch_predict = train_rgb.Save_predictions(save_pred)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer_func,loss=loss_func)\n",
    "\n",
    "    fls = len(os.listdir(load_training))\n",
    "\n",
    "                                #train_dir, patch_size, batch_size\n",
    "    train_generator = train_rgb.train_generator_rgb(load_training,ptch_size,32)\n",
    "    history = model.fit_generator(generator = train_generator,steps_per_epoch=3*fls, verbose = 1,\n",
    "                                 epochs = 200,callbacks = [tbCallBack,csv_logger,epoch_predict])\n",
    "    print(history.history)\n",
    "\n",
    "    model.save(os.path.join(save_file,'DeMos_mod.h5'))\n",
    "    \n",
    "    #Test Kodak\n",
    "    data ={}\n",
    "\n",
    "    kodak_dir = r'C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak'\n",
    "    ls = len(os.listdir(kodak_dir))\n",
    "\n",
    "    kodak_generator = train_rgb.predict_generator_rgb(kodak_dir)\n",
    "\n",
    "    # k_pred = model.predict_generator(kodak_generator, steps = ls)\n",
    "\n",
    "    res_Kodak = train_rgb.predict_generator(model,kodak_generator,ls,data)\n",
    "\n",
    "    data['Kodak_IMGS_PSNR'] = res_Kodak[0]\n",
    "    data['Kodak_IMGS_SSIM'] = res_Kodak[1]\n",
    "    data['Kodak_AVG_PSNR']  = res_Kodak[2]\n",
    "    data['Kodak_AVG_SSIM']  = res_Kodak[3]\n",
    "    \n",
    "    #Test McManus\n",
    "    McM_dir = r'C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM'\n",
    "    ls = len(os.listdir(McM_dir))\n",
    "\n",
    "    McM_generator = train_rgb.predict_generator_rgb(McM_dir)\n",
    "\n",
    "    # k_pred = model.predict_generator(kodak_generator, steps = ls)\n",
    "\n",
    "    res_McM = train_rgb.predict_generator(model,McM_generator,ls,data)\n",
    "\n",
    "    data['McM_IMGS_PSNR'] = res_McM[0]\n",
    "    data['McM_IMGS_SSIM'] = res_McM[1]\n",
    "    data['McM_AVG_PSNR']  = res_McM[2]\n",
    "    data['McM_AVG_SSIM']  = res_McM[3]\n",
    "    \n",
    "    #Write Results\n",
    "    data['Parameters'] = {\n",
    "        'Loss Function': loss_func,\n",
    "        'Optimizer':str(type(optimizer_func))\n",
    "    }\n",
    "    data['Training Set'] = {\n",
    "        'Training Path': load_training,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(save_file,'results.txt'), 'w') as outfile:  \n",
    "        json.dump(data, outfile,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim01.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim02.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim03.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim04.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim05.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim06.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim07.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim08.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim09.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim10.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim11.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim12.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim13.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim14.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim15.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim16.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim17.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim18.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim19.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim20.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim21.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim22.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim23.png\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak\\kodim24.png\n",
      "Kodak PSNR Average: 37.2373114132\n",
      "Kodak SSIM Average: 0.978890099157\n"
     ]
    }
   ],
   "source": [
    "#Test Kodak\n",
    "data ={}\n",
    "\n",
    "kodak_dir = r'C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\Kodak'\n",
    "ls = len(os.listdir(kodak_dir))\n",
    "\n",
    "kodak_generator = train_rgb.predict_generator_rgb(kodak_dir)\n",
    "\n",
    "# k_pred = model.predict_generator(kodak_generator, steps = ls)\n",
    "\n",
    "res_Kodak = train_rgb.predict_generator(model,kodak_generator,ls,data,5,save_test)\n",
    "\n",
    "data['Kodak_IMGS_PSNR'] = res_Kodak[0]\n",
    "data['Kodak_IMGS_SSIM'] = res_Kodak[1]\n",
    "data['Kodak_AVG_PSNR']  = res_Kodak[2]\n",
    "data['Kodak_AVG_SSIM']  = res_Kodak[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\1.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\10.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\11.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\12.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\13.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\14.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\15.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\16.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\17.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\18.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\2.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\3.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\4.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\5.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\6.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\7.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\8.tif\n",
      "C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM\\9.tif\n",
      "Kodak PSNR Average: 34.4612706074\n",
      "Kodak SSIM Average: 0.934323556834\n"
     ]
    }
   ],
   "source": [
    "#Test McManus\n",
    "McM_dir = r'C:\\Users\\buggyr\\Mosaic_Experiments\\data\\interim\\McM'\n",
    "ls = len(os.listdir(McM_dir))\n",
    "\n",
    "McM_generator = train_rgb.predict_generator_rgb(McM_dir)\n",
    "\n",
    "# k_pred = model.predict_generator(kodak_generator, steps = ls)\n",
    "\n",
    "res_McM = train_rgb.predict_generator(model,McM_generator,ls,data,5,save_test)\n",
    "\n",
    "data['McM_IMGS_PSNR'] = res_McM[0]\n",
    "data['McM_IMGS_SSIM'] = res_McM[1]\n",
    "data['McM_AVG_PSNR']  = res_McM[2]\n",
    "data['McM_AVG_SSIM']  = res_McM[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write Results\n",
    "data['Parameters'] = {\n",
    "    'Loss Function': loss_func,\n",
    "    'Optimizer':str(type(optimizer_func))\n",
    "}\n",
    "data['Training Set'] = {\n",
    "    'Training Path': load_training,\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_file,'results.txt'), 'w') as outfile:  \n",
    "    json.dump(data, outfile,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'train_rgb' from 'C:\\\\Users\\\\buggyr\\\\Mosaic_Experiments\\\\src\\\\train_rgb.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(train_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r'C:\\Users\\buggyr\\Mosaic_Experiments\\models\\2017-11-22 12-53 RGB_ptch128\\DeMos_mod.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(os.path.join(save_file,'DeMos_mod.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file =r'C:\\Users\\buggyr\\Mosaic_Experiments\\models\\2017-11-22 12-53 RGB_ptch128'\n",
    "save_pred = os.path.join(save_file,'Epoch_Predictions')\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir=os.path.join(save_file,'TNSR_BRD'), histogram_freq=0, write_graph=True, write_images=True)\n",
    "csv_logger = keras.callbacks.CSVLogger(os.path.join(save_file,'training.log'), separator=',', append=False)\n",
    "epoch_predict = train_rgb.Save_predictions(save_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
